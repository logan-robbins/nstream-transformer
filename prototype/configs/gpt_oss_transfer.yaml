model:
  hidden_size: 4096
  vocab_size: 32000
  notes_dim: 2048
  num_heads: 32
  trunk:
    base_model: "gpt-oss-20b/original"
    torch_dtype: "bfloat16"
    device_map: "auto"
    freeze_lower_layers: 4
    unfreeze_modules:
      - "lm_head"
      - "adapter"
  role_adapters:
    hidden_size: 4096
    bottleneck_size: 512
    roles:
      - "intro"
      - "core"
      - "wrap"
    activation: "gelu"
    dropout: 0.05
  notes_bus:
    snapshot_dim: 2048
    max_snapshots: 4
    lag: 1
    dtype: "bfloat16"
  cross_attention:
    hidden_size: 4096
    notes_dim: 2048
    num_heads: 32
    gating_init: -4.0
  planner_head:
    hidden_size: 4096
    vocab_size: 32000
    dropout: 0.0
  notes_head:
    hidden_size: 4096
    notes_dim: 2048
    dropout: 0.0
    gated: true
  speculation_head:
    hidden_size: 4096
    notes_dim: 2048
    dropout: 0.0
    teacher_scale: 1.0
  agreement_head:
    hidden_size: 4096
    dropout: 0.1
training:
  dataset_path: "data/processed/wiki-v6-500/kd.jsonl"
  eval_dataset_path: null
  telemetry_dir: "experiments/gpt_oss"
  batch_size: 1
  grad_accumulation: 1
  learning_rate: 1.0e-4
  weight_decay: 0.0
  max_steps: 1000
  warmup_steps: 0
  log_interval: 10
  eval_interval: 200
  device: null
  teacher:
    enabled: true
    type: "stop_grad"
    ema_decay: 0.995
  teacher_runner:
    provider: "ollama"
    model: "llama3.1:70b-instruct-q5_K_M"
    api_key: null
    planner_provider: "ollama"
    planner_model: "llama3.1:70b-instruct-q5_K_M"
    planner_api_key: null
    cache_dir: "data/teacher_cache"
    id_field: "example_id"
    document_field: "document_text"
    paragraph_field: "document_paragraphs"
    plan_field: "teacher_plan"
    roles:
      - "intro"
      - "core"
      - "wrap"
    max_snapshots: 4
    stride_window: 128
    refresh_cache: false
    notes_per_snapshot: 1
    embedder_model: "sentence-transformers/all-MiniLM-L6-v2"
  curriculum:
    B: 4
    L: 32
    delta: 1
    rho_by_role: {}
    steps_per_stage: 0
    stage_schedule:
      - 0
      - 500
      - 1500
      - 3000
      - 4500
  stage_policies:
    0:
      name: "planner_pretrain"
      freeze:
        - "trunk"
        - "role_adapters"
        - "cross_attention"
        - "notes_bus"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
        - "role_classifier"
      unfreeze:
        - "planner_head"
        - "notes_head"
    1:
      name: "role_bootstrap"
      freeze:
        - "trunk"
        - "notes_bus"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "role_adapters"
        - "cross_attention"
        - "role_classifier"
    2:
      name: "notes_bus_enable"
      freeze:
        - "trunk"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "notes_bus"
        - "speculation_head"
    3:
      name: "rollback_training"
      bus_mix_prob: 0.25
      role_dropout_prob: 0.1
      freeze:
        - "trunk"
      unfreeze:
        - "agreement_head"
        - "coverage_head"
    4:
      name: "stability_supervision"
      bus_mix_prob: 0.5
      role_dropout_prob: 0.15
      notes_noise:
        drop_p: 0.05
        paraphrase_p: 0.1
      unfreeze:
        - "trunk"
  loss_weights:
    kd: 1.0
    stab: 0.1
    use: 0.0
    cov: 0.2
    nli: 0.05
    red: 0.0
    spec_kl: 0.1
    role: 0.0
    agree: 0.0
  coverage_threshold: 0.5
  bus_mix_prob: 0.0
  role_dropout_prob: 0.0
  parallel_micro_steps: 0
  notes_noise:
    drop_p: 0.0
    paraphrase_p: 0.0
  negative_sampling:
    enabled: false
    start_stage: 3
    contradiction_ratio: 0.5
    max_contradictions: 4
    noise_ratio: 0.1
    noise_std: 0.05
  nli_scorer: null
  metrics:
    mask_ablation_every: 100
    stability_every: 100
  gradnorm:
    enabled: false
    target_ratio: 1.0
    alpha: 0.05
    min_scale: 0.1
    max_scale: 5.0
  nli_margin: 0.1
  spec_kl_temperature: 1.0
