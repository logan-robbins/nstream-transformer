model:
  hidden_size: 1024
  vocab_size: 32000
  notes_dim: 256
  num_heads: 8
  plan_vocab_size: 4096
  trunk:
    base_model: "gpt-oss-20b/original"
    torch_dtype: "float32"
    device_map: "cpu"
    freeze_lower_layers: 0
    unfreeze_modules:
      - "lm_head"
      - "adapter"
  role_adapters:
    hidden_size: 1024
    bottleneck_size: 128
    roles:
      - "intro"
      - "core"
      - "wrap"
    activation: "gelu"
    dropout: 0.1
  notes_bus:
    snapshot_dim: 256
    max_snapshots: 4
    lag: 1
    dtype: "float32"
  cross_attention:
    hidden_size: 1024
    notes_dim: 256
    num_heads: 8
    gating_init: -3.0
  planner_head:
    hidden_size: 1024
    vocab_size: 32000
    dropout: 0.1
  notes_head:
    hidden_size: 1024
    notes_dim: 256
    dropout: 0.0
    gated: true
  speculation_head:
    hidden_size: 1024
    notes_dim: 256
    dropout: 0.0
    teacher_scale: 1.0
  agreement_head:
    hidden_size: 1024
    dropout: 0.1
training:
  seed: 1234
  dataset_path: "data/processed/wiki_smoke/kd.jsonl"
  eval_dataset_path: null
  telemetry_dir: "experiments/wiki_smoke"
  batch_size: 1
  grad_accumulation: 1
  learning_rate: 2.5e-4
  weight_decay: 0.0
  max_steps: 24
  warmup_steps: 0
  log_interval: 4
  eval_interval: 8
  device: "cpu"
  teacher:
    enabled: true
    type: "stop_grad"
    ema_decay: 0.995
  teacher_runner:
    provider: "ollama"
    model: "llama3.1:70b-instruct-q5_K_M"
    api_key: null
    planner_provider: "ollama"
    planner_model: "llama3.1:70b-instruct-q5_K_M"
    planner_api_key: null
    cache_dir: "data/teacher_cache"
    id_field: "example_id"
    document_field: "document_text"
    paragraph_field: "document_paragraphs"
    plan_field: "teacher_plan"
    roles:
      - "intro"
      - "core"
      - "wrap"
    max_snapshots: 4
    stride_window: 64
    refresh_cache: false
    notes_per_snapshot: 1
    embedder_model: "sentence-transformers/all-MiniLM-L6-v2"
  curriculum:
    B: 2
    L: 16
    delta: 1
    rho_by_role:
      intro: 2
      core: 2
      wrap: 2
    steps_per_stage: 0
    stage_schedule:
      - 0
      - 6
      - 12
      - 18
      - 22
  stage_policies:
    0:
      name: "planner_bootstrap"
      freeze:
        - "trunk"
        - "role_adapters"
        - "cross_attention"
        - "notes_bus"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
        - "role_classifier"
      unfreeze:
        - "planner_head"
        - "notes_head"
    1:
      name: "adapt_roles"
      freeze:
        - "trunk"
        - "notes_bus"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "role_adapters"
        - "cross_attention"
    2:
      name: "bus_alignment"
      freeze:
        - "trunk"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "notes_bus"
        - "speculation_head"
    3:
      name: "rollback_training"
      bus_mix_prob: 0.25
      role_dropout_prob: 0.05
      freeze:
        - "trunk"
      unfreeze:
        - "agreement_head"
        - "coverage_head"
    4:
      name: "stability_supervision"
      bus_mix_prob: 0.4
      role_dropout_prob: 0.1
      notes_noise:
        drop_p: 0.05
        paraphrase_p: 0.05
      unfreeze:
        - "trunk"
  loss_weights:
    kd: 0.2
    stab: 0.05
    use: 0.0
    cov: 0.1
    nli: 0.0
    red: 0.0
    spec_kl: 0.1
    role: 0.0
    agree: 0.0
  coverage_threshold: 0.5
  bus_mix_prob: 0.0
  role_dropout_prob: 0.0
  parallel_micro_steps: 0
  notes_noise:
    drop_p: 0.0
    paraphrase_p: 0.0
  negative_sampling:
    enabled: false
    start_stage: 3
    contradiction_ratio: 0.0
    max_contradictions: 2
    noise_ratio: 0.0
    noise_std: 0.02
  metrics:
    mask_ablation_every: 12
    stability_every: 12
  gradnorm:
    enabled: false
    target_ratio: 1.0
    alpha: 0.05
    min_scale: 0.1
    max_scale: 5.0
  nli_margin: 0.1
  spec_kl_temperature: 1.0
  agreement_threshold: 0.2
